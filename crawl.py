# Generated by ChatGPT
import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Set up headers for the request to avoid being blocked by anti-bot mechanisms
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'
}

# Function to download a file
def download_file(url, folder):
    try:
        response = requests.get(url, headers=HEADERS, stream=True)
        if response.status_code == 200:
            filename = url.split('/')[-1]
            filepath = os.path.join(folder, filename)
            with open(filepath, 'wb') as file:
                for chunk in response.iter_content(chunk_size=1024):
                    file.write(chunk)
            print(f"Downloaded: {filename}")
        else:
            print(f"Failed to download {url} - Status code: {response.status_code}")
    except Exception as e:
        print(f"Error downloading {url}: {e}")

# Function to extract and download PDFs from the webpage
def scrape_and_download_pdfs(base_url, output_folder):
    try:
        response = requests.get(base_url, headers=HEADERS)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all links on the page
        links = soup.find_all('a', href=True)

        # Filter for PDF links
        pdf_links = [urljoin(base_url, link['href']) for link in links if link['href'].endswith('.pdf')]

        if not os.path.exists(output_folder):
            os.makedirs(output_folder)

        # Download each PDF
        for pdf_link in pdf_links:
            download_file(pdf_link, output_folder)

        print("All PDFs downloaded successfully.")
    except requests.exceptions.RequestException as e:
        print(f"Error accessing the page: {e}")
    except Exception as e:
        print(f"Error during scraping: {e}")

# Define the main function
def main():
    base_url = "https://www.unwomen.org/en/how-we-work/commission-on-the-status-of-women/csw69-2025/preparations"
    output_folder = "downloaded_pdfs"

    scrape_and_download_pdfs(base_url, output_folder)

# Run the script
if __name__ == "__main__":
    main()
